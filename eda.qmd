---
title: "Project title"
subtitle: "Exploratory data analysis"
format: html
editor: visual
execute:
  echo: true
---

# Research question(s)

Research question(s). State your research question (s) clearly.

Is there a correlation between the height and/or weight and specific game statistics (Including: goals scored, assists, penalties, passes, etc.)

-   Do teams with taller players tend to have more penalties?

Are some leagues better than others?

-   Are some leagues outputting more goals than others?

-   Which team with the greatest goal output in every league has the most penalties (i.e. yellow/red cards)?

# Data collection and cleaning

Have an initial draft of your data cleaning appendix. Document every step that takes your raw data file(s) and turns it into the analysis-ready data set that you would submit with your final project. Include text narrative describing your data collection (downloading, scraping, surveys, etc) and any additional data curation/cleaning (merging data frames, filtering, transformations of variables, etc). Include code for data curation/cleaning, but not collection.

```{r}
#|: ex1
library(tidyverse)
library(skimr)
library(janitor)

soccer <- read_csv("data/top_goalscorers.csv")
soccer <- clean_names(soccer)

soccer_clean <- soccer |>
  mutate(player_height_cm = as.numeric(gsub(" cm", "", soccer$player_height)),
         player_weight_kg = as.numeric(gsub(" kg", "", soccer$player_weight))) |>
  select(-c(games_number, goals_saves, dribbles_past, penalty_won, penalty_commited, player_height, player_weight, penalty_saved, player_id_1, player_id_48, player_firstname))

soccer_clean
```

The data was first collected by making API calls from https://www.api-football.com/documentation-v3 for various soccer leagues. We did this my making a GET request to their top scorers endpoint, supplying parameters for each of 27 top leagues in 2021. From this we collected 20 goal scorers for each respective league as we specified per the query string parameters. From this point, we used the jsonlite library to flatten the JSON API responses into a tibble and then exported the tibble to a CSV file.

Once we had our data stored in a CSV file, we wrote code to load it in R and then began the cleaning process. The main reason why we stored the data to a CSV rather than dynamically making API calls is due to the rate limits and fees associated with using the API. For the main cleaning process, we utilized the clean_names function from the janitor library to make sure our variable names conform to R naming conventions. After this, we dropped the columns we would not be using by employing a select statement on our data frame. Originally, our data set had columns that were relevant to goalkeepers, defenders, and other players that are not primarily goalscorers. We also noticed that our height and weight columns were not sanitized and in character form versus numeric, thus we sanitized these columns added the units to the variable name and dropped the old columns. Lastly, we made sure to remove any columns that appeared to be duplicates like player_id_48, player_id_1, and player_firstname.

# Data description

Have an initial draft of your data description section. Your data description should be about your analysis-ready data.

```{r}
View(soccer1)
glimpse(soccer1)
```

-   What are the observations (rows) and the attributes (columns)?

    -   There are 560 observations and 56 attributes (columns). The dataset contains player statistics (ex/ number of appearances, number of minutes played, number of yellow or red cards attained) for the top 20 scorers from the top 27 football leagues for the 2021 league season.

-   Why was this dataset created?

    -   This dataset was created to rank and compare the top scoring football players across top leagues. This could be helpful for football fans, sports betters, or data scientists!

-   Who funded the creation of the dataset?

    -   N/a (Most stats are automatically recorded, but were probably scraped from some summary site)

-   What processes might have influenced what data was observed and recorded and what was not?

    -   It may have been influenced by referee rulings, as some scores may not have counted in the final game statistics.

-   What preprocessing was done, and how did the data come to be in the form that you are using?

    -   ***JOHN:***

-   If people are involved, were they aware of the data collection and if so, what purpose did they expect the data to be used for?

    -   The people involved are football players, who are aware of data collection, a this occurs during and after all of their games.

    -   They probably expected the data to be used for:

        -   general comparison (past/future versions of themselves, comparing their team to another, or between leagues)

        -   sports reporting (to football fans)

        -   MVP decisions

# Data limitations

-   Data is only from the 2021 season. Therefore, we cannot make comparisons within groups (player, team, or league) over time.

-   There are no goalies in the dataset (best scorers i.e. offensive players) therefore, we are unable to assess a team's defense, which is important when considering how good a team is!

# Exploratory data analysis

Perform an (initial) exploratory data analysis.

```{r}
#|: exploratory-data-analysis 

library(tidyverse)
library(tidymodels)

#Example From ae14
# Let's start by taking a look at our data. Create an density plot to investigate the relationship between `spam` and `exclaim_mess`. Additionally, calculate the mean number of exclamation points for both spam and non-spam emails.

# ggplot(data = email, mapping = aes(x = exclaim_mess, fill = spam)) +
#   geom_density(alpha = 0.5) + 
#   scale_x_sqrt()
```

# Questions for reviewers

-   How many research questions should we have-- if only one or two, which ones do you suggest?

-   The data we are using is already pretty tidy (tidied when converting from API --\> JSON --\> CSV), therefore there are minimal cleaning steps in R\-- confirming that this is acceptable, as it was indicated in the proposal directions that using an API was OK.
